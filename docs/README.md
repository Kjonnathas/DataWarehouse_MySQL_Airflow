![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![MySQL](https://img.shields.io/badge/mysql-4479A1.svg?style=for-the-badge&logo=mysql&logoColor=white)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)

# 1. Descri√ß√£o do Projeto

Este projeto foi desenvolvido com o objetivo de criar uma estrutura robusta de Data Warehouse, abrangendo desde a modelagem entidade-relacionamento (ER) at√© as etapas de modelagem conceitual, l√≥gica e f√≠sica. O trabalho incluiu a cria√ß√£o de um banco de dados transacional (OLTP) e a implementa√ß√£o de um pipeline ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) para integrar os dados ao Data Warehouse (DW).

O fluxo de dados foi estruturado da seguinte forma:

1. Banco de Dados Transacional (OLTP): Armazenamento inicial dos dados em uma estrutura otimizada para transa√ß√µes.

2. Staging Area: Um ambiente intermedi√°rio para a integra√ß√£o e pr√©-processamento dos dados extra√≠dos.

3. Data Warehouse: Reposit√≥rio final onde os dados transformados s√£o organizados para an√°lises.

Este projeto demonstra o processo completo de desenvolvimento de um Data Warehouse, desde a modelagem inicial at√© a automa√ß√£o de pipelines ETL, proporcionando uma base s√≥lida para an√°lises de dados e suporte √† tomada de decis√µes.

# 2. Objetivo do Projeto

Desenvolver uma solu√ß√£o completa de Data Warehouse que integre modelagem de dados, cria√ß√£o de um banco transacional (OLTP) e automa√ß√£o de pipelines ETL, utilizando tecnologias modernas como Docker, Airbyte e Apache Airflow. O projeto visa demonstrar, na pr√°tica, o fluxo completo de dados desde a origem at√© a consolida√ß√£o em um ambiente otimizado para an√°lises estrat√©gicas, com foco em escalabilidade, portabilidade e organiza√ß√£o eficiente dos dados. Al√©m disso, busca aplicar os conhecimentos adquiridos na forma√ß√£o de Engenheiro de Dados da Data Science Academy, servindo como inspira√ß√£o para que outros profissionais possam utilizar esta arquitetura em seus pr√≥prios projetos, tanto pessoais quanto profissionais.

# 3. Arquitetura do Projeto

![Arquitetura do Projeto Data Warehouse](/assets/images/arquitetura_projeto.png)

# 4. Tecnologias Utilizadas

- Git
- SQL
- PostgreSQL
- MySQL
- Python
- Airbyte
- Apache Airflow
- Docker
- UML

# 5. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica da Arquitetura

1.  Cria√ß√£o do Banco de Dados Transacional (OLTP) üè¶

    - Foram gerados diversos arquivos CSV contendo os dados de entrada.
    - Um script Python foi desenvolvido para realizar o carregamento autom√°tico desses arquivos no MySQL, configurado como o banco de dados transacional (OLTP).
    - O banco OLTP foi implementado em um servidor dedicado, executado dentro de um cont√™iner Docker, e consistia em cerca de 30 tabelas criadas previamente atrav√©s de scripts SQL executados diretamente no cliente do MySQL.

2.  Staging Area üõ†Ô∏è

    - Para a staging area, foi configurado outro servidor com PostgreSQL, tamb√©m rodando em um cont√™iner Docker independente.
    - A ferramenta Airbyte foi utilizada para estabelecer a conex√£o entre o banco OLTP (MySQL) e a staging area (PostgreSQL). Essa etapa foi respons√°vel pela extra√ß√£o e carga inicial dos dados, garantindo a separa√ß√£o l√≥gica entre os ambientes.

3.  Transforma√ß√£o e Carga no Data Warehouse (DW) üìä

    - O Apache Airflow foi configurado para orquestrar o pipeline de ETL, utilizando uma DAG personalizada que:

      - Extrai os dados da staging area no PostgreSQL.

      - Transforma os dados aplicando as regras de neg√≥cio necess√°rias.
      - Carrega os dados no Data Warehouse, que tamb√©m foi implementado em PostgreSQL, mas em um servidor e cont√™iner Docker dedicados.

    - As tabelas do DW foram previamente criadas atrav√©s de scripts SQL no cliente do PostgreSQL, garantindo que a estrutura estivesse alinhada ao modelo de dados definido para o projeto.

4.  Automatiza√ß√£o e Integra√ß√£o üîÑ

    - Todo o processo de ETL foi automatizado, permitindo a execu√ß√£o cont√≠nua e eficiente do fluxo de dados entre os diferentes ambientes.

Este projeto n√£o apenas simula uma arquitetura real de Data Warehouse como tamb√©m aplica conceitos fundamentais de engenharia de dados e automa√ß√£o de pipelines. üêçüêò‚öôÔ∏è

# 6. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica das Ferramentas

1. <strong> üêç Python </strong>

   O Python foi utilizado para desenvolver scripts que automatizam o carregamento dos dados no MySQL, que funcionou como banco de dados transacional (OLTP) com cerca de 30 tabelas. Os arquivos CSV gerados foram processados e carregados via scripts Python, que atuaram como a ponte entre os dados e o banco de dados, garantindo que as informa√ß√µes fossem corretamente inseridas nas tabelas do MySQL. Al√©m disso, o Python tamb√©m foi utilizado para criar a DAG (Directed Acyclic Graph) no Apache Airflow, orquestrando o fluxo de trabalho do processo de ETL.

2. <strong> üê≥ Docker Desktop </strong>

   O Docker Desktop foi utilizado para criar cont√™ineres isolados e garantir que cada servi√ßo funcionasse em um ambiente controlado e separado. Criei um cont√™iner com o MySQL, que foi respons√°vel pelo banco de dados transacional (OLTP), e um cont√™iner separado com o PostgreSQL para simular a staging area. Al√©m disso, o PostgreSQL foi utilizado no Data Warehouse (DW), que tamb√©m foi configurado dentro de um cont√™iner Docker em um servidor separado. O uso de cont√™ineres facilitou a gest√£o dos servi√ßos e a escalabilidade do projeto.

3. <strong> ü™º Airbyte </strong>

   O Airbyte foi utilizado como ferramenta de integra√ß√£o de dados, conectando a fonte de dados (MySQL) √† staging area (PostgreSQL). Ele foi respons√°vel por extrair os dados do banco de dados MySQL e transferi-los para o PostgreSQL, garantindo que os dados estivessem prontos para serem processados e transformados no fluxo de ETL. O Airbyte automatizou esse processo de movimenta√ß√£o de dados, garantindo a consist√™ncia e a atualiza√ß√£o das informa√ß√µes.

4. <strong> ‚òÅÔ∏è Apache Airflow </strong>

   O Apache Airflow foi a plataforma escolhida para orquestrar todo o processo de ETL. Criei uma DAG (Directed Acyclic Graph) no Airflow para automatizar o fluxo de trabalho. A DAG buscava os dados da staging area no PostgreSQL, aplicava as transforma√ß√µes necess√°rias e, em seguida, carregava os dados transformados no Data Warehouse (DW), que tamb√©m estava hospedado no PostgreSQL. O Airflow garantiu que as etapas do processo de transforma√ß√£o e carregamento dos dados fossem executadas de forma sequencial e programada.

5. <strong> üê¨ MySQL </strong>

   O MySQL foi utilizado como o banco de dados transacional (OLTP) respons√°vel pelo armazenamento das tabelas operacionais com cerca de 30 tabelas. O MySQL recebeu os dados de entrada, que foram carregados por meio de scripts Python. Esse banco de dados foi fundamental para o armazenamento e gerenciamento dos dados brutos, que depois seriam processados e transformados para an√°lises posteriores.

6. <strong> üêò PostgreSQL </strong>

   O PostgreSQL foi utilizado de duas maneiras: como a staging area e como o banco de dados do Data Warehouse (DW). Na staging area, o PostgreSQL recebeu os dados extra√≠dos do MySQL via Airbyte, e no Data Warehouse, o PostgreSQL foi utilizado para armazenar os dados transformados, permitindo consultas anal√≠ticas e a gera√ß√£o de insights de forma eficiente. Antes do processo de ETL come√ßar, as tabelas foram criadas no PostgreSQL com scripts SQL, que definiram a estrutura do banco de dados.

# 7. Exibi√ß√£o do Projeto

# 8. Instala√ß√£o e Configura√ß√£o (Replica√ß√£o)

# 9. Licen√ßa
