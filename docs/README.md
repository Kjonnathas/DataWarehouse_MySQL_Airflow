![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![MySQL](https://img.shields.io/badge/mysql-4479A1.svg?style=for-the-badge&logo=mysql&logoColor=white)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)

# 1. Descri√ß√£o do Projeto

Este projeto foi desenvolvido com o objetivo de criar uma estrutura robusta de Data Warehouse, abrangendo desde a implementa√ß√£o da modelagem conceitual, l√≥gica e f√≠sica. O trabalho incluiu a cria√ß√£o de um banco de dados transacional (OLTP) e a implementa√ß√£o de um pipeline ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) para integrar os dados ao Data Warehouse (DW).

O fluxo de dados foi estruturado da seguinte forma:

1. Banco de Dados Transacional (OLTP): Armazenamento inicial dos dados em uma estrutura otimizada para transa√ß√µes.

2. Staging Area: Um ambiente intermedi√°rio para a integra√ß√£o e pr√©-processamento dos dados extra√≠dos.

3. Data Warehouse: Reposit√≥rio final onde os dados transformados s√£o organizados para an√°lises.

Este projeto demonstra o processo completo de desenvolvimento de um Data Warehouse, desde a modelagem inicial at√© a automa√ß√£o de pipelines ETL, proporcionando uma base s√≥lida para an√°lises de dados e suporte √† tomada de decis√µes.

# 2. Objetivo do Projeto

Desenvolver uma solu√ß√£o completa de Data Warehouse que integre modelagem de dados, cria√ß√£o de um banco transacional (OLTP) e automa√ß√£o de pipelines ETL, utilizando tecnologias modernas como Docker, Airbyte e Apache Airflow. O projeto visa demonstrar, na pr√°tica, o fluxo completo de dados desde a origem at√© a consolida√ß√£o em um ambiente otimizado para an√°lises estrat√©gicas, com foco em escalabilidade, portabilidade e organiza√ß√£o eficiente dos dados. Al√©m disso, busca aplicar os conhecimentos adquiridos na forma√ß√£o de Engenheiro de Dados da Data Science Academy, servindo como inspira√ß√£o para que outros profissionais possam utilizar esta arquitetura em seus pr√≥prios projetos, tanto pessoais quanto profissionais.

# 3. Arquitetura do Projeto

![Arquitetura do Projeto Data Warehouse](/assets/images/arquitetura_projeto.png)

# 4. Modelagem de Dados

## 4.1 Modelo Conceitual (Diagrama Entidade-Relacionamento - DER)

![Modelo Conceitual](/assets/images/modelo_conceitual.png)

## 4.2 Modelo L√≥gico

![Modelo L√≥gico](/assets/images/modelo_logico.png)

## 4.3 Modelo Dimensional

![Modelo Dimensional](/assets/images/modelo_dimensional.png)

# 5. Tecnologias Utilizadas

- Git
- SQL
- PostgreSQL
- MySQL
- Python
- Airbyte
- Apache Airflow
- Docker
- UML

# 6. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica da Arquitetura

1.  Cria√ß√£o do Banco de Dados Transacional (OLTP) üè¶

    - Foram gerados diversos arquivos CSV contendo os dados de entrada.
    - Um script Python foi desenvolvido para realizar o carregamento autom√°tico desses arquivos no MySQL, configurado como o banco de dados transacional (OLTP).
    - O banco OLTP foi implementado em um servidor dedicado, executado dentro de um cont√™iner Docker, e consistia em cerca de 30 tabelas criadas previamente atrav√©s de scripts SQL executados diretamente no cliente do MySQL.

2.  Staging Area üõ†Ô∏è

    - Para a staging area, foi configurado outro servidor com PostgreSQL, tamb√©m rodando em um cont√™iner Docker independente.
    - A ferramenta Airbyte foi utilizada para estabelecer a conex√£o entre o banco OLTP (MySQL) e a staging area (PostgreSQL). Essa etapa foi respons√°vel pela extra√ß√£o e carga inicial dos dados, garantindo a separa√ß√£o l√≥gica entre os ambientes.

3.  Transforma√ß√£o e Carga no Data Warehouse (DW) üìä

    - O Apache Airflow foi configurado para orquestrar o pipeline de ETL, utilizando uma DAG personalizada que:

      - Extrai os dados da staging area no PostgreSQL.

      - Transforma os dados aplicando as regras de neg√≥cio necess√°rias.
      - Carrega os dados no Data Warehouse, que tamb√©m foi implementado em PostgreSQL, mas em um servidor e cont√™iner Docker dedicados.

    - As tabelas do DW foram previamente criadas atrav√©s de scripts SQL no cliente do PostgreSQL, garantindo que a estrutura estivesse alinhada ao modelo de dados definido para o projeto.

4.  Automatiza√ß√£o e Integra√ß√£o üîÑ

    - Todo o processo de ETL foi automatizado, permitindo a execu√ß√£o cont√≠nua e eficiente do fluxo de dados entre os diferentes ambientes.

Este projeto n√£o apenas simula uma arquitetura real de Data Warehouse como tamb√©m aplica conceitos fundamentais de engenharia de dados e automa√ß√£o de pipelines. üêçüêò‚öôÔ∏è

# 7. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica das Ferramentas

1. <strong> üêç Python </strong>

   O Python foi utilizado para desenvolver scripts que automatizam o carregamento dos dados no MySQL, que funcionou como banco de dados transacional (OLTP) com cerca de 30 tabelas. Os arquivos CSV gerados foram processados e carregados via scripts Python, que atuaram como a ponte entre os dados e o banco de dados, garantindo que as informa√ß√µes fossem corretamente inseridas nas tabelas do MySQL. Al√©m disso, o Python tamb√©m foi utilizado para criar a DAG (Directed Acyclic Graph) no Apache Airflow, orquestrando o fluxo de trabalho do processo de ETL.

2. <strong> üê≥ Docker Desktop </strong>

   O Docker Desktop foi utilizado para criar cont√™ineres isolados e garantir que cada servi√ßo funcionasse em um ambiente controlado e separado. Criei um cont√™iner com o MySQL, que foi respons√°vel pelo banco de dados transacional (OLTP), e um cont√™iner separado com o PostgreSQL para simular a staging area. Al√©m disso, o PostgreSQL foi utilizado no Data Warehouse (DW), que tamb√©m foi configurado dentro de um cont√™iner Docker em um servidor separado. O uso de cont√™ineres facilitou a gest√£o dos servi√ßos e a escalabilidade do projeto.

3. <strong> ü™º Airbyte </strong>

   O Airbyte foi utilizado como ferramenta de integra√ß√£o de dados, conectando a fonte de dados (MySQL) √† staging area (PostgreSQL). Ele foi respons√°vel por extrair os dados do banco de dados MySQL e transferi-los para o PostgreSQL, garantindo que os dados estivessem prontos para serem processados e transformados no fluxo de ETL. O Airbyte automatizou esse processo de movimenta√ß√£o de dados, garantindo a consist√™ncia e a atualiza√ß√£o das informa√ß√µes.

4. <strong> ‚òÅÔ∏è Apache Airflow </strong>

   O Apache Airflow foi a plataforma escolhida para orquestrar todo o processo de ETL. Criei uma DAG (Directed Acyclic Graph) no Airflow para automatizar o fluxo de trabalho. A DAG buscava os dados da staging area no PostgreSQL, aplicava as transforma√ß√µes necess√°rias e, em seguida, carregava os dados transformados no Data Warehouse (DW), que tamb√©m estava hospedado no PostgreSQL. O Airflow garantiu que as etapas do processo de transforma√ß√£o e carregamento dos dados fossem executadas de forma sequencial e programada.

5. <strong> üê¨ MySQL </strong>

   O MySQL foi utilizado como o banco de dados transacional (OLTP) respons√°vel pelo armazenamento das tabelas operacionais com cerca de 30 tabelas. O MySQL recebeu os dados de entrada, que foram carregados por meio de scripts Python. Esse banco de dados foi fundamental para o armazenamento e gerenciamento dos dados brutos, que depois seriam processados e transformados para an√°lises posteriores.

6. <strong> üêò PostgreSQL </strong>

   O PostgreSQL foi utilizado de duas maneiras: como a staging area e como o banco de dados do Data Warehouse (DW). Na staging area, o PostgreSQL recebeu os dados extra√≠dos do MySQL via Airbyte, e no Data Warehouse, o PostgreSQL foi utilizado para armazenar os dados transformados, permitindo consultas anal√≠ticas e a gera√ß√£o de insights de forma eficiente. Antes do processo de ETL come√ßar, as tabelas foram criadas no PostgreSQL com scripts SQL, que definiram a estrutura do banco de dados.

# 8. Exibi√ß√£o do Projeto

## 8.1 Cont√™ineres Docker

![C√¥nteires Docker](/assets/images/docker.png)

## 8.2 Banco de Dados Transacional no MySQL

### 8.2.1 Tabelas do Banco de Dados

![Banco de Dados MySQL](/assets/images/mysql_1.png)

### 8.2.2 Tabela de Produtos

![Tabela de Produtos no MySQL](/assets/images/mysql_2.png)

### 8.2.3 Tabela de Pedidos

![Tabela de Pedidos no MySQL](/assets/images/mysql_3.png)

### 8.2.4 Tabela de Endere√ßos

![Tabela de Endere√ßos no MySQL](/assets/images/mysql_4.png)

## 8.3 Airbyte

### 8.3.1 Airbyte Source (Fonte de Dados)

![Source do Airbyte](/assets/images/airbyte_1.png)

### 8.3.2 Airbyte Destination (Destino dos Dados)

![Destination do Airbyte](/assets/images/airbyte_2.png)

### 8.3.3 Airbyte Connections (Conex√£o dos Dados)

![Connection do Airbyte](/assets/images/airbyte_3.png)

![Connection do Airbyte](/assets/images/airbyte_4.png)

## 8.4 √Årea Intermedi√°ria e Banco de Dados Anal√≠tico no PostgreSQL

### 8.4.1 Servidores PostgreSQL

![Servidores PostgreSQL](/assets/images/postgresql_1.png)

### 8.4.2 Servidor da Staging Area

![Servidor da Staging Area](/assets/images/postgresql_2.png)

### 8.4.3 Tabelas no Schema da Staging Area

![Tabelas no Schema da Staging Area](/assets/images/postgresql_3.png)

### 8.4.4 Servidor do Data Warehouse (DW)

![Servidor do Data Warehouse](/assets/images/postgresql_4.png)

### 8.4.5 Tabelas no Schema do Data Warehouse

![Tabelas no Schema do Data Warehouse](/assets/images/postgresql_5.png)

### 8.4.6 Tabela Fato Vendas

![Tabela Fato Vendas](/assets/images/postgresql_6.png)

### 8.4.7 Tabela Dimens√£o Cliente

![Tabela Dimens√£o Cliente](/assets/images/postgresql_7.png)

## 8.5 Airflow

### 8.5.1 √Årea de DAGS

![√Årea de DAGS no Airflow](/assets/images/airflow_1.png)

### 8.5.2 Fluxo da DAG

![Fluxo da DAG no Airflow](/assets/images/airflow_2.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_3.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_4.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_5.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_6.png)

# 9. Instala√ß√£o e Configura√ß√£o (Replica√ß√£o)

- Pr√©-requisitos:

  1. Baixar e instalar o Docker Desktop;

     Link para download: https://www.docker.com/

  2. Baixar e instalar o PgAdmin (Client do PostgreSQL);

     Link para download: https://www.pgadmin.org/download/

  3. Baixar e instalar o Workbench (Client do MySQL);

     Link para download: https://dev.mysql.com/downloads/workbench/

  4. Baixar e instalar o Airbyte;

     Link para download: https://docs.airbyte.com/using-airbyte/getting-started/oss-quickstart

  5. Baixar e instalar o Git;

     Link para download: https://git-scm.com/downloads

<br>

- Passo a passo:

1. Primeira coisa que precisamos fazer √© instalar o python. Para isso, vamos utilizar o Pyenv que √© um pacote que permite gerenciar diversas vers√µes do Python na mesma m√°quina.

```bash
git clone https://github.com/pyenv-win/pyenv-win.git %USERPROFILE%\.pyenv
```

2. Ap√≥s executar o comando acima, ser√° criada uma pasta no seu usu√°rio chamada .pyenv, que conter√° todos os arquivos necess√°rios para que o Pyenv possa funcionar. Agora, precisamos configurar vari√°veis de ambiente para que o sistema operacional consiga compreender os comandos do Pyenv no prompt de comando. V√° at√© a lupa de pesquisa e procure por "configura√ß√µes avan√ßadas do sistema". Em seguida, clique em "Vari√°veis de ambiente" e posteriormente clique em "Novo" na parte de vari√°veis do usu√°rio. Ap√≥s isso, abrir√° uma janela para incluir o nome e o valor da vari√°vel de ambiente que est√° logo abaixo.

```bash
PYENV=C:\Users\seu_usuario\.pyenv\pyenv-win\
PYENV_HOME=C:\Users\seu_usuario\.pyenv\pyenv-win\
PYENV_ROOT=C:\Users\seu_usuario\.pyenv\pyenv-win\
```

3. Para ter certeza que o Pyenv est√° instalado e funcionando corretamente, abra o prompt de comando e digite:

```bash
pyenv --version
```

4. Se ap√≥s a execu√ß√£o do c√≥digo acima retornar a vers√£o do Pyenv √© porque o sistema operacional j√° est√° conseguindo compreender os comandos. Caso contr√°rio, revise o passo a passo descrito e se for necess√°rio busque ajuda no Google. Por conseguinte, agora com as configura√ß√µes necess√°rias, precisamos instalar o Python atrav√©s do Pyenv.

```bash
pyenv install 3.12.1
```

5. Com o Python instalado, precisamos agora clonar o projeto. Caso queira manter o projeto em uma pasta espec√≠fica, voc√™ pode navegar atrav√©s da estrutura de pastas do seu notebook/computador com o comando "cd nome_pasta". Se n√£o especificar a pasta, provavelmente o projeto ser√° criado na sua pasta de usu√°rio.Feito isso, digite:

```bash
git clone https://github.com/Kjonnathas/DataWarehouse_MySQL_Airflow.git
```

6. Com o projeto clonado, precisamos especificar a vers√£o do python que ir√° rodar no projeto. Para isso, execute o seguinte comando dentro da pasta do projeto:

```bash
pyenv local 3.12.1
```

7. Em seguida, vamos criar nosso ambiente virtual. O ambiente virtual √© uma boa pr√°tica que visa manter um ambiente √∫nico para cada projeto desenvolvido. Sendo assim, execute o comando:

```bash
python -m venv .venv
```

8. Agora precisamos ativar o ambiente virtual. Para isso, execute:

```bash
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser

.venv/Scripts/Activate.ps1
```

9. Depois de ter feito o passo anterior, √© preciso agora realizar a instala√ß√£o das bibliotecas que s√£o utilizadas no projeto. Portanto, dentro da pasta "DataWarehouse_MySQL_Airflow", use o seguinte comando:

```bash
pip install -r requirements.txt
```

10. Nesse momento, podemos come√ßar a cria√ß√£o do cont√©iner que rodar√° o servidor do nosso banco de dados transacional via Docker. Abra o Docker Desktop. Em seguida, abra o prompt de comando caso ainda n√£o esteja aberto ou tenha fechado e execute:

```bash
docker pull mysql:8.0-debian
```

11. O passo acima far√° o download da imagem do MySQL do Docker Hub. Agora, com o download feito, podemos criar o cont√©iner. Depois de executar o comando abaixo, abra o Docker Desktop e veja se o cont√©iner foi criado.

```bash
docker run --name mysql_prod -p 3307:3306 -e MYSQL_ROOT_PASSWORD=escolha_uma_senha -d mysql:8.0-debian
```

12. Vamos aproveitar e criar logo tamb√©m o servidor que usaremos para armazenar a nossa staging area. Ap√≥s executar, fa√ßa o mesmo check no Docker Desktop para averiguar se o cont√©iner foi criado direitinho.

```bash
docker run --name postgres_stg --network bridge -p 5433:5432 -e POSTGRES_DB=escolha_nome_do_banco -e POSTGRES_USER=escolha_nome_do_usuario -e POSTGRES_PASSWORD=escolha_uma_senha -d postgres
```

13. Com o servidor do banco de dados transacional e da staging area criada, podemos avan√ßar na cria√ß√£o das tabelas no banco de dados transacional. Por√©m, antes de criar as tabelas, precisamos configurar o servidor. Para isso, abra o Workbench (Client do MySQL). Ap√≥s abrir, clique no bot√£o de "+" ao lado de "MySQL Connections". Com isso, abrir√° uma janela para inserir as configura√ß√µes. Insira um nome para a conex√£o (da sua escolha), o host - para descobrir o host voc√™ pode executar o seguinte comando no prompt de comando "docker inspect mysql_prod". Ir√° abrir uma estrutura de json e v√° at√© o final e procure por "IPAddress". O valor desse campo √© o host da m√°quina -, a porta (3307) e a senha. Depois disso teste a conex√£o e veja se deu certo. Se der certo, basta clicar sobre a conex√£o que ir√° direcion√°-lo para a tela principal do client.

14. Depois de ter feito o passo anterior, v√° at√© o arquivo "database_transacional.sql" que est√° dentro da pasta "models" que est√° dentro da pasta "src" e copie todo o conte√∫do. Copiado o conte√∫do, volte ao Workbench e cole. Depois selecione todo o conte√∫do colado e execute (CTRL + Enter ou clique no bot√£o de run). Em seguida, fa√ßa um refresh e veja se o banco de dados e as tabelas foram criadas normalmente.

15. Agora que temos o banco de dados e as tabelas criadas, vamos precisar executar um script Python para popular as tabelas que estar√£o simulando nosso ambiente transacional. Procure pelo script "job_etl.py" dentro da pasta "jobs" que est√° dentro da pasta "src". Navegue at√© esta pasta atrav√©s do seu terminal. Voc√™ pode fazer isso usando o seguinte comando:

```bash
cd src/jobs
```

16. Antes de executarmos o script Python, precisamos criar um arquivo ".env" para armazenar algumas vari√°veis de ambiente. Sem elas, o script resultar√° em erro. Portanto, crie este arquivo dentro da pasta "DataWarehouse_MySQL_Airflow" e insira as seguintes informa√ß√µes:

    - MYSQL_USER=root
    - MYSQL_PASSWORD=sua_senha
    - MYSQL_DB=seu_banco_de_dados
    - MYSQL_HOST=seu_host
    - MYSQL_PORT=3307
    - FOLDER_PATH=insira_o_caminho_completo_da_pasta_data
    - FOLDER_LOGS=insira_o_caminho_completo_da_pasta_logs

17. Depois de ter entrado na pasta jobs e configurado o arquivo ".env", basta apenas executar o c√≥digo Python para que ele possa ler os arquivos csv e depois carreg√°-los no banco de dados. Para isso, digite no seu terminal:

```bash
python job_etl.py
```

18. No banco de dados da staging, a √∫nica coisa que precisa ser feita neste momento √© a cria√ß√£o do schema. Para isso, voc√™ pode tanto criar manualmente quanto via c√≥digo. Manualmente, bast√° ir at√© o objeto "Schemas" dentro da hierarquia do banco de dados e clicar com o bot√£o direito escolhendo a op√ß√£o "Create" e em seguida "Schema...". Com isso, abrir√° uma janela onde voc√™ incluir√° o nome do Schema na caixa de texto da op√ß√£o "Name" e posteriormente clique em "Save". Com isso, o seu schema ser√° criado. Caso prefira a op√ß√£o via c√≥digo SQL, apenas abra o query tool clicando novamente sobre o objeto Schema e em "Query Tool" e digite "CREATE SCHEMA nome_do_schema" e execute apertando a tecla F5 ou no bot√£o de run e o schema ser√° criado da mesma forma;

19. Se o passo anterior deu certo, podemos avan√ßar na cria√ß√£o do cont√©iner do Airbyte e suas configura√ß√µes. Aqui √© importante que voc√™ j√° tenha feito o download do arquivo que √© disponibilizado pelo Airbyte na pr√≥pria documenta√ß√£o. O arquivo ser√° baixado em uma pasta zipada, portanto, fa√ßa a descompacta√ß√£o da pasta ap√≥s o download ter sido conclu√≠do. Feito isso, precisamos adicionar uma vari√°vel de ambiente para que o sistema operacional reconhe√ßa os comandos do Airbyte. Para isso, v√° at√© suas vari√°veis de ambiente (explicado mais acima como se chega) e procure pela vari√°vel de ambiente "Path" na parte de vari√°veis de ambiente de usu√°rio. Depois que encontrar, clique sobre ela e em seguida em "Editar". Ap√≥s isso, abrir√° uma janela com todas os caminhos associados ao "Path". O que voc√™ precisa fazer √© pegar o caminho completo da pasta do Airbyte e adicionar na √∫ltima linha dispon√≠vel. Depois que o fizer, apenas clique em "Ok" e saia da tela de vari√°veis de ambiente. Caso esteja com o prompt de comando aberto, feche-o e abra novamente.

20. Depois dessas configura√ß√µes, execute o comando abaixo para virmos se o sistema operacional j√° est√° reconhecendo os comandos do Airbyte.

```bash
abctl version
```

21. Caso o prompt de comando retorne a vers√£o do Airbyte significa que est√° tudo certo e podemos prosseguir para a pr√≥xima etapa, que √© cria√ß√£o do cont√©iner do Airbyte. Para isso, precisa estar com o Docker Desktop aberto. Com ele aberto, v√° at√© o seu prompt de comando e digite:

```bash
abctl local install
```

22. Essa etapa costuma demorar pela primeira vez, ent√£o tenha paci√™ncia! √â importante verificar na documenta√ß√£o a exig√™ncia m√≠nima de hardware, pois o Airbyte √© pesado e consome bastante mem√≥ria. Se seu computador n√£o for muito bom, √© prov√°vel que v√° ter problemas com travamento.

23. Ap√≥s a instala√ß√£o ter sido conclu√≠da com sucesso, voc√™ pode abrir seu Docker Desktop e verificar se o cont√©iner do Airbyte foi criado com sucesso. Caso tenha sido, digite o comando abaixo:

```bash
abctl local credentials
```

24. O comando acima lhe dar√° um ID e uma senha, que ser√£o necess√°rios para logar no Airbyte via navegador. Teste a sua conex√£o indo at√© o navegador e acessando "localhost:8000". Insira suas credenciais e o Airbyte lhe levar√° para a tela principal da plataforma.

25. Com o cont√©iner do Airbyte criado precisamos fazer uma configura√ß√£o de rede para que ele consiga se comunicar com os outros cont√©ineres criados anteriormente (do postgresql e do mysql). Se voc√™ retornar na etapa que criamos esses outros dois cont√©neires notar√° que passamos um par√¢metro chamado "--network" e esse par√¢metro √© respons√°vel por dizer em qual rede o cont√©iner estar√° localizado. Portanto, vamos fazer o mesmo agora para o Airbyte. Siga os comandos abaixo em sequ√™ncia. Substituia <rede_atual> pela rede que Airbyte est√° e tamb√©m substitua <container_name> pelo nome do cont√©iner que certamente deve ser "airbyte-abctl-control-plane". O primeiro comando mostrar√° a voc√™ a rede do cont√©iner. O nome da rede est√° antes dos dois pontos e dentro do par de colchetes. Se voc√™ copiar todo o resultado e substituir em <rede_atual> resultar√° em erro. Portanto, pegue exatamente a parte que eu descrevi.

```bash
docker inspect -f '{{.NetworkSettings.Networks}}' <container_name>

docker network disconnect <rede_atual> <container_name>

docker network connect bridge <container_name>
```

26. Com o ambiente do Airbyte agora configurado, precisamos apenas criar a source, o destination e a connection para que a ETL seja capaz de extrair os dados do servidor de produ√ß√£o e lev√°-los ao ambiente da staging area. No painel lateral esquerdo, clique em "Sources". Em seguida, clique em "New source". Na caixa de buscas, procure por "mysql" e clique sobre o conector do MySQL. Agora, aparecer√° uma s√©rie de configura√ß√µes que precisam ser feitas:

    a. Em "Source name" d√™ um nome para a sua fonte de dados;

    b. Em "Host" voc√™ precisa incluir o endere√ßo IP da m√°quina do cont√©iner, usando o comando "docker inspect <container_name>" voc√™ consegue encontrar na parte final da estrutura json que √© retornada. Adicionalmente, voc√™ pode testar se o nome do cont√©iner tamb√©m funciona nessa etapa;

    c. Em "Port" voc√™ precisa colocar a porta mapeada pro cont√©iner. Como a comunica√ß√£o s√£o entre cont√©neires, ou seja, uma comunica√ß√£o apenas no mundo interno, voc√™ ir√° colocar a porta 3306;

    d. Em "Database" precisamos colocar o nome do banco de dados e caso voc√™ n√£o tenha alterado, o nome ser√° "db_prod". N√£o coloque as aspas;

    d. Em "Username" voc√™ vai p√¥r "root". Novamente n√£o inclua as aspas;

    e. Em "Password" inclua a senha que foi definida durante a cria√ß√£o do cont√©iner;

    f. Em "SSL modes" escolha a op√ß√£o "preferred";

    g. Em "Update Method" escolha a op√ß√£o "Scan Changes with User Defined Cursor";

    h. Em "SSH Tunnel Method" escolha a op√ß√£o "No Tunnel".

Feita todas as configura√ß√µes clique em "Test and save". Se o teste concluir sem qualquer erro a source est√° criada.

27. Agora vamos criar e configurar a etapa de Destination. Para isso, clique em "Destinations" no menu lateral esquerdo. Em seguida clique em "New destination". No campo de pesquisa procure por "postgres" e clique no conector. Novamente aparecer√° uma s√©rie de configura√ß√µes que precisam ser realizadas:

    a. Em "Destination name" escolha um nome para o seu destino;

    b. Em "Host" voc√™ seguir√° a mesma explica√ß√£o que foi dada na etapa de configura√ß√£o da Source. Aqui, obviamente, o host ser√° diferente, tanto se voc√™ optar pelo endere√ßo IP quanto atrav√©s do nome do cont√©iner;

    c. Em "Port" voc√™ colocar√° a porta 5432;

    d. Em "DB Name" ter√° que incluir o nome do banco de dados que voc√™ escolheu no momento de criar o cont√©iner do postgresql;

    e. Em "Default Schema" apague o schema public que vem por padr√£o e inclua o schema que foi criado;

    f. Em "User" voc√™ indicar√° o nome do usu√°rio;

    g. Em "SSL modes" escolher√° a op√ß√£o "disable";

    h. Em "SSH Tunnel Method" escolher√° a op√ß√£o "No Tunnel";

    i. Em "Password" voc√™ definir√° a senha escolhida no momento de cria√ß√£o do cont√©iner.

Feita todas as configura√ß√µes clique em "Test and save". Se o teste concluir sem qualquer erro a destination est√° criada.

28. Por fim, vamos criar a nossa connection. Para isso, v√° at√© o menu lateral esquerdo e clique sobre "Connections". Posteriormente clique em "New connection". Nesse momento, o Airbyte apresentar√° as sources que voc√™ tem configurada. Ent√£o, escolha a source que criamos anteriormente. Em seguida, precisamos definir qual ser√° a destination. Similarmente a etapa anterior, basta escolher a destination que criamos e seguir em frente. Com isso, o Airbyte far√° o carregamento e nos dar√° a op√ß√£o de escolher os "streams", que nada mais s√£o do que as tabelas do banco de dados. Selecione todas as tabelas e na parte de "Sync mode" escolha a op√ß√£o "Full refresh | Overwrite + Deduped". Na aba de "Settings" apenas defina o nome da connection e em "schedule type" defina como "Manual". Depois, basta habilitar a connection na parte superior direita e executar o processo clicando em "Sync now";

29. Se a etapa anterior funcionar, vamos seguir em frente para cria√ß√£o do servidor do Data Warehouse, cria√ß√£o das tabelas do DW e ajuste e configura√ß√£o do Airflow para fazer o Orquestramento do nosso pipeline de dados;

30.

# 10. Licen√ßa

Este projeto est√° licenciado sob os termos da licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

```mermaid
graph TD
A[In√≠cio] --> B[Carregar vari√°veis de ambiente com dotenv]
B --> C[Configurar logging]
C --> D[Fun√ß√£o normalize_text]
D --> E[Fun√ß√£o read_files]
E --> F[Fun√ß√£o connection_database]
F --> G[Fun√ß√£o create_table_and_insert_data]
G --> H[Executar script principal]
H --> I[Estabelecer conex√£o com o banco de dados]
I --> J{Conex√£o bem-sucedida?}
J -- Sim --> K[Cria√ß√£o de tabelas e inser√ß√£o de dados]
J -- N√£o --> L[Registrar erro no log]
K --> M{Existem arquivos CSV na pasta?}
M -- Sim --> N[Processar cada arquivo]
N --> O[Normalizar e formatar dados]
O --> P[Inserir dados no banco]
M -- N√£o --> Q[Registrar aviso no log]
P --> R[Commit na transa√ß√£o]
R --> S[Fim]
Q --> S
L --> S
```
