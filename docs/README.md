![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![MySQL](https://img.shields.io/badge/mysql-4479A1.svg?style=for-the-badge&logo=mysql&logoColor=white)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)

# 1. Descri√ß√£o do Projeto

Este projeto foi desenvolvido com o objetivo de criar uma estrutura robusta de Data Warehouse, abrangendo desde a implementa√ß√£o da modelagem conceitual, l√≥gica e f√≠sica. O trabalho incluiu a cria√ß√£o de um banco de dados transacional (OLTP) e a implementa√ß√£o de um pipeline ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) para integrar os dados ao Data Warehouse (DW).

O fluxo de dados foi estruturado da seguinte forma:

1. Banco de Dados Transacional (OLTP): Armazenamento inicial dos dados em uma estrutura otimizada para transa√ß√µes.

2. Staging Area: Um ambiente intermedi√°rio para a integra√ß√£o e pr√©-processamento dos dados extra√≠dos.

3. Data Warehouse: Reposit√≥rio final onde os dados transformados s√£o organizados para an√°lises.

Este projeto demonstra o processo completo de desenvolvimento de um Data Warehouse, desde a modelagem inicial at√© a automa√ß√£o de pipelines ETL, proporcionando uma base s√≥lida para an√°lises de dados e suporte √† tomada de decis√µes.

# 2. Objetivo do Projeto

Desenvolver uma solu√ß√£o completa de Data Warehouse que integre modelagem de dados, cria√ß√£o de um banco transacional (OLTP) e automa√ß√£o de pipelines ETL, utilizando tecnologias modernas como Docker, Airbyte e Apache Airflow. O projeto visa demonstrar, na pr√°tica, o fluxo completo de dados desde a origem at√© a consolida√ß√£o em um ambiente otimizado para an√°lises estrat√©gicas, com foco em escalabilidade, portabilidade e organiza√ß√£o eficiente dos dados. Al√©m disso, busca aplicar os conhecimentos adquiridos na forma√ß√£o de Engenheiro de Dados da Data Science Academy, servindo como inspira√ß√£o para que outros profissionais possam utilizar esta arquitetura em seus pr√≥prios projetos, tanto pessoais quanto profissionais.

# 3. Arquitetura do Projeto

![Arquitetura do Projeto Data Warehouse](/assets/images/arquitetura_projeto.png)

# 4. Modelagem de Dados

## 4.1 Modelo Conceitual (Diagrama Entidade-Relacionamento - DER)

![Modelo Conceitual](/assets/images/modelo_conceitual.png)

## 4.2 Modelo L√≥gico

![Modelo L√≥gico](/assets/images/modelo_logico.png)

## 4.3 Modelo Dimensional

![Modelo Dimensional](/assets/images/modelo_dimensional.png)

# 5. Tecnologias Utilizadas

- Git
- SQL
- PostgreSQL
- MySQL
- Python
- Airbyte
- Apache Airflow
- Docker
- UML

# 6. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica da Arquitetura

1.  Cria√ß√£o do Banco de Dados Transacional (OLTP) üè¶

    - Foram gerados diversos arquivos CSV contendo os dados de entrada.
    - Um script Python foi desenvolvido para realizar o carregamento autom√°tico desses arquivos no MySQL, configurado como o banco de dados transacional (OLTP).
    - O banco OLTP foi implementado em um servidor dedicado, executado dentro de um cont√™iner Docker, e consistia em cerca de 30 tabelas criadas previamente atrav√©s de scripts SQL executados diretamente no cliente do MySQL.

2.  Staging Area üõ†Ô∏è

    - Para a staging area, foi configurado outro servidor com PostgreSQL, tamb√©m rodando em um cont√™iner Docker independente.
    - A ferramenta Airbyte foi utilizada para estabelecer a conex√£o entre o banco OLTP (MySQL) e a staging area (PostgreSQL). Essa etapa foi respons√°vel pela extra√ß√£o e carga inicial dos dados, garantindo a separa√ß√£o l√≥gica entre os ambientes.

3.  Transforma√ß√£o e Carga no Data Warehouse (DW) üìä

    - O Apache Airflow foi configurado para orquestrar o pipeline de ETL, utilizando uma DAG personalizada que:

      - Extrai os dados da staging area no PostgreSQL.

      - Transforma os dados aplicando as regras de neg√≥cio necess√°rias.
      - Carrega os dados no Data Warehouse, que tamb√©m foi implementado em PostgreSQL, mas em um servidor e cont√™iner Docker dedicados.

    - As tabelas do DW foram previamente criadas atrav√©s de scripts SQL no cliente do PostgreSQL, garantindo que a estrutura estivesse alinhada ao modelo de dados definido para o projeto.

4.  Automatiza√ß√£o e Integra√ß√£o üîÑ

    - Todo o processo de ETL foi automatizado, permitindo a execu√ß√£o cont√≠nua e eficiente do fluxo de dados entre os diferentes ambientes.

Este projeto n√£o apenas simula uma arquitetura real de Data Warehouse como tamb√©m aplica conceitos fundamentais de engenharia de dados e automa√ß√£o de pipelines. üêçüêò‚öôÔ∏è

# 7. Descri√ß√£o de como as Tecnologias foram utilizadas atrav√©s da √≥tica das Ferramentas

1. <strong> üêç Python </strong>

   O Python foi utilizado para desenvolver scripts que automatizam o carregamento dos dados no MySQL, que funcionou como banco de dados transacional (OLTP) com cerca de 30 tabelas. Os arquivos CSV gerados foram processados e carregados via scripts Python, que atuaram como a ponte entre os dados e o banco de dados, garantindo que as informa√ß√µes fossem corretamente inseridas nas tabelas do MySQL. Al√©m disso, o Python tamb√©m foi utilizado para criar a DAG (Directed Acyclic Graph) no Apache Airflow, orquestrando o fluxo de trabalho do processo de ETL.

2. <strong> üê≥ Docker Desktop </strong>

   O Docker Desktop foi utilizado para criar cont√™ineres isolados e garantir que cada servi√ßo funcionasse em um ambiente controlado e separado. Criei um cont√™iner com o MySQL, que foi respons√°vel pelo banco de dados transacional (OLTP), e um cont√™iner separado com o PostgreSQL para simular a staging area. Al√©m disso, o PostgreSQL foi utilizado no Data Warehouse (DW), que tamb√©m foi configurado dentro de um cont√™iner Docker em um servidor separado. O uso de cont√™ineres facilitou a gest√£o dos servi√ßos e a escalabilidade do projeto.

3. <strong> ü™º Airbyte </strong>

   O Airbyte foi utilizado como ferramenta de integra√ß√£o de dados, conectando a fonte de dados (MySQL) √† staging area (PostgreSQL). Ele foi respons√°vel por extrair os dados do banco de dados MySQL e transferi-los para o PostgreSQL, garantindo que os dados estivessem prontos para serem processados e transformados no fluxo de ETL. O Airbyte automatizou esse processo de movimenta√ß√£o de dados, garantindo a consist√™ncia e a atualiza√ß√£o das informa√ß√µes.

4. <strong> ‚òÅÔ∏è Apache Airflow </strong>

   O Apache Airflow foi a plataforma escolhida para orquestrar todo o processo de ETL. Criei uma DAG (Directed Acyclic Graph) no Airflow para automatizar o fluxo de trabalho. A DAG buscava os dados da staging area no PostgreSQL, aplicava as transforma√ß√µes necess√°rias e, em seguida, carregava os dados transformados no Data Warehouse (DW), que tamb√©m estava hospedado no PostgreSQL. O Airflow garantiu que as etapas do processo de transforma√ß√£o e carregamento dos dados fossem executadas de forma sequencial e programada.

5. <strong> üê¨ MySQL </strong>

   O MySQL foi utilizado como o banco de dados transacional (OLTP) respons√°vel pelo armazenamento das tabelas operacionais com cerca de 30 tabelas. O MySQL recebeu os dados de entrada, que foram carregados por meio de scripts Python. Esse banco de dados foi fundamental para o armazenamento e gerenciamento dos dados brutos, que depois seriam processados e transformados para an√°lises posteriores.

6. <strong> üêò PostgreSQL </strong>

   O PostgreSQL foi utilizado de duas maneiras: como a staging area e como o banco de dados do Data Warehouse (DW). Na staging area, o PostgreSQL recebeu os dados extra√≠dos do MySQL via Airbyte, e no Data Warehouse, o PostgreSQL foi utilizado para armazenar os dados transformados, permitindo consultas anal√≠ticas e a gera√ß√£o de insights de forma eficiente. Antes do processo de ETL come√ßar, as tabelas foram criadas no PostgreSQL com scripts SQL, que definiram a estrutura do banco de dados.

# 8. Exibi√ß√£o do Projeto

## 8.1 Cont√™ineres Docker

![C√¥nteires Docker](/assets/images/docker.png)

## 8.2 Banco de Dados Transacional no MySQL

### 8.2.1 Tabelas do Banco de Dados

![Banco de Dados MySQL](/assets/images/mysql_1.png)

### 8.2.2 Tabela de Produtos

![Tabela de Produtos no MySQL](/assets/images/mysql_2.png)

### 8.2.3 Tabela de Pedidos

![Tabela de Pedidos no MySQL](/assets/images/mysql_3.png)

### 8.2.4 Tabela de Endere√ßos

![Tabela de Endere√ßos no MySQL](/assets/images/mysql_4.png)

## 8.3 Airbyte

### 8.3.1 Airbyte Source (Fonte de Dados)

![Source do Airbyte](/assets/images/airbyte_1.png)

### 8.3.2 Airbyte Destination (Destino dos Dados)

![Destination do Airbyte](/assets/images/airbyte_2.png)

### 8.3.3 Airbyte Connections (Conex√£o dos Dados)

![Connection do Airbyte](/assets/images/airbyte_3.png)

![Connection do Airbyte](/assets/images/airbyte_4.png)

## 8.4 √Årea Intermedi√°ria e Banco de Dados Anal√≠tico no PostgreSQL

### 8.4.1 Servidores PostgreSQL

![Servidores PostgreSQL](/assets/images/postgresql_1.png)

### 8.4.2 Servidor da Staging Area

![Servidor da Staging Area](/assets/images/postgresql_2.png)

### 8.4.3 Tabelas no Schema da Staging Area

![Tabelas no Schema da Staging Area](/assets/images/postgresql_3.png)

### 8.4.4 Servidor do Data Warehouse (DW)

![Servidor do Data Warehouse](/assets/images/postgresql_4.png)

### 8.4.5 Tabelas no Schema do Data Warehouse

![Tabelas no Schema do Data Warehouse](/assets/images/postgresql_5.png)

### 8.4.6 Tabela Fato Vendas

![Tabela Fato Vendas](/assets/images/postgresql_6.png)

### 8.4.7 Tabela Dimens√£o Cliente

![Tabela Dimens√£o Cliente](/assets/images/postgresql_7.png)

## 8.5 Airflow

### 8.5.1 √Årea de DAGS

![√Årea de DAGS no Airflow](/assets/images/airflow_1.png)

### 8.5.2 Fluxo da DAG

![Fluxo da DAG no Airflow](/assets/images/airflow_2.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_3.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_4.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_5.png)

![Fluxo da DAG no Airflow](/assets/images/airflow_6.png)

# 9. Instala√ß√£o e Configura√ß√£o (Replica√ß√£o)

- Pr√©-requisitos:

  1. Baixar e instalar o Docker Desktop;

     Link para download: https://www.docker.com/

  2. Baixar e instalar o PgAdmin (Client do PostgreSQL);

     Link para download: https://www.pgadmin.org/download/

  3. Baixar e instalar o Workbench (Client do MySQL);

     Link para download: https://dev.mysql.com/downloads/workbench/

  4. Baixar e instalar o Airbyte;

     Link para download: https://docs.airbyte.com/using-airbyte/getting-started/oss-quickstart

  5. Baixar e instalar o Git;

     Link para download: https://git-scm.com/downloads

<br>

- Passo a passo:

1. Primeira coisa que precisamos fazer √© instalar o python. Para isso, vamos utilizar o Pyenv que √© um pacote que permite gerenciar diversas vers√µes do Python na mesma m√°quina.

```
git clone https://github.com/pyenv-win/pyenv-win.git %USERPROFILE%\.pyenv
```

2. Ap√≥s executar o comando acima, ser√° criada uma pasta no seu usu√°rio chamada .pyenv, que conter√° todos os arquivos necess√°rios para que o Pyenv possa funcionar. Agora, precisamos configurar vari√°veis de ambiente para que o sistema operacional consiga compreender os comandos do Pyenv no prompt de comando. V√° at√© a lupa de pesquisa e procure por "configura√ß√µes avan√ßadas do sistema". Em seguida, clique em "Vari√°veis de ambiente" e posteriormente clique em "Novo" na parte de vari√°veis do usu√°rio. Ap√≥s isso, abrir√° uma janela para incluir o nome e o valor da vari√°vel de ambiente que est√° logo abaixo.

```
PYENV=C:\Users\seu_usuario\.pyenv\pyenv-win\
PYENV_HOME=C:\Users\seu_usuario\.pyenv\pyenv-win\
PYENV_ROOT=C:\Users\seu_usuario\.pyenv\pyenv-win\
```

3. Para ter certeza que o Pyenv est√° instalado e funcionando corretamente, abra o prompt de comando e digite:

```
pyenv --version
```

4. Se ap√≥s a execu√ß√£o do c√≥digo acima retornar a vers√£o do Pyenv √© porque o sistema operacional j√° est√° conseguindo compreender os comandos. Caso contr√°rio, revise o passo a passo descrito e se for necess√°rio busque ajuda no Google. Por conseguinte, agora com as configura√ß√µes necess√°rias, precisamos instalar o Python atrav√©s do Pyenv.

```
pyenv install 3.12.1
```

5. Com o Python instalado, precisamos agora clonar o projeto. Caso queira manter o projeto em uma pasta espec√≠fica, voc√™ pode navegar atrav√©s da estrutura de pastas do seu notebook/computador com o comando "cd nome_pasta". Se n√£o especificar a pasta, provavelmente o projeto ser√° criado na sua pasta de usu√°rio.Feito isso, digite:

```
git clone https://github.com/Kjonnathas/DataWarehouse_MySQL_Airflow.git
```

6. Com o projeto clonado, precisamos especificar a vers√£o do python que ir√° rodar no projeto. Para isso, execute o seguinte comando dentro da pasta do projeto:

```
pyenv local 3.12.1
```

7. Em seguida, vamos criar nosso ambiente virtual. O ambiente virtual √© uma boa pr√°tica que visa manter um ambiente √∫nico para cada projeto desenvolvido. Sendo assim, execute o comando:

```
python -m venv .venv
```

8. Agora precisamos ativar o ambiente virtual. Para isso, execute:

```
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser

.venv/Scripts/Activate.ps1
```

9. Depois de ter feito o passo anterior, √© preciso agora realizar a instala√ß√£o das bibliotecas que s√£o utilizadas no projeto. Portanto, dentro da pasta "DataWarehouse_MySQL_Airflow", use o seguinte comando:

```
pip install -r requirements.txt
```

10. Nesse momento, podemos come√ßar a cria√ß√£o do cont√©iner que rodar√° o servidor do nosso banco de dados transacional via Docker. Abra o Docker Desktop. Em seguida, abra o prompt de comando caso ainda n√£o esteja aberto ou tenha fechado e execute:

```
docker pull mysql:8.0-debian
```

11. O passo acima far√° o download da imagem do MySQL do Docker Hub. Agora, com o download feito, podemos criar o cont√©iner. Depois de executar o comando abaixo, abra o Docker Desktop e veja se o cont√©iner foi criado.

```
docker run --name mysql_prod -p 3307:3306 -e MYSQL_ROOT_PASSWORD=escolha_uma_senha -d mysql:8.0-debian
```

12. Vamos aproveitar e criar logo tamb√©m o servidor que usaremos para armazenar a nossa staging area. Ap√≥s executar, fa√ßa o mesmo check no Docker Desktop para averiguar se o cont√©iner foi criado direitinho.

```
docker run --name postgres_stg --network bridge -p 5433:5432 -e POSTGRES_DB=escolha_nome_do_banco -e POSTGRES_USER=escolha_nome_do_usuario -e POSTGRES_PASSWORD=escolha_uma_senha -d postgres
```

13. Com o servidor do banco de dados transacional e da staging area criada, podemos avan√ßar na cria√ß√£o das tabelas no banco de dados transacional. Por√©m, antes de criar as tabelas, precisamos configurar o servidor. Para isso, abra o Workbench (Client do MySQL). Ap√≥s abrir, clique no bot√£o de "+" ao lado de "MySQL Connections". Com isso, abrir√° uma janela para inserir as configura√ß√µes. Insira um nome para a conex√£o (da sua escolha), o host - para descobrir o host voc√™ pode executar o seguinte comando no prompt de comando "docker inspect mysql_prod". Ir√° abrir uma estrutura de json e v√° at√© o final e procure por "IPAddress". O valor desse campo √© o host da m√°quina -, a porta (3307) e a senha. Depois disso teste a conex√£o e veja se deu certo. Se der certo, basta clicar sobre a conex√£o que ir√° direcion√°-lo para a tela principal do client.

14. Depois de ter feito o passo anterior, v√° at√© o arquivo "database_transacional.sql" que est√° dentro da pasta "models" que est√° dentro da pasta "src" e copie todo o conte√∫do. Copiado o conte√∫do, volte ao Workbench e cole. Depois selecione todo o conte√∫do colado e execute (CTRL + Enter ou clique no bot√£o de run). Em seguida, fa√ßa um refresh e veja se o banco de dados e as tabelas foram criadas normalmente.

15. Agora que temos o banco de dados e as tabelas criadas, vamos precisar executar um script Python para popular as tabelas que estar√£o simulando nosso ambiente transacional. Procure pelo script "job_etl.py" dentro da pasta "jobs" que est√° dentro da pasta "src". Navegue at√© esta pasta atrav√©s do seu terminal. Voc√™ pode fazer isso usando o seguinte comando:

```
cd src/jobs
```

16. Antes de executarmos o script Python, precisamos criar um arquivo ".env" para armazenar algumas vari√°veis de ambiente. Sem elas, o script resultar√° em erro. Portanto, crie este arquivo dentro da pasta "DataWarehouse_MySQL_Airflow" e insira as seguintes informa√ß√µes:

    - MYSQL_USER=root
    - MYSQL_PASSWORD=sua_senha
    - MYSQL_DB=seu_banco_de_dados
    - MYSQL_HOST=seu_host
    - MYSQL_PORT=3307
    - FOLDER_PATH=insira_o_caminho_completo_da_pasta_data
    - FOLDER_LOGS=insira_o_caminho_completo_da_pasta_logs

17. Depois de ter entrado na pasta jobs e configurado o arquivo ".env", basta apenas executar o c√≥digo Python para que ele possa ler os arquivos csv e depois carreg√°-los no banco de dados. Para isso, digite no seu terminal:

```
python job_etl.py
```

18. Se o passo anterior deu certo, podemos avan√ßar na cria√ß√£o do cont√©iner do Airbyte e suas configura√ß√µes.

# 10. Licen√ßa

Este projeto est√° licenciado sob os termos da licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
